{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-21T11:47:09.543266Z",
     "start_time": "2025-01-21T11:47:08.061091Z"
    }
   },
   "source": [
    "# 导入需要的包\n",
    "# Import the required packages.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T11:47:16.015957Z",
     "start_time": "2025-01-21T11:47:15.988826Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 数据读取 input data\n",
    "# train_dir = \"/bohr/dataset-sxb8/v1/train.csv\"\n",
    "train_dir = \"train.csv\"\n",
    "df_train = pd.read_csv(train_dir)"
   ],
   "id": "45eb55928e9b5b6",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T11:47:17.251258Z",
     "start_time": "2025-01-21T11:47:17.233403Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 数据预处理与数据嵌入；Data Preprocessing and Data Embedding\n",
    "# 由于该过程在预测集上也需要进行，此处将其整理为函数；Since this process also needs to be performed on the prediction set, it is organized into a function here.\n",
    "# 选手可充分考虑不同数据嵌入技术，以提高预测效果；Participants are encouraged to fully consider different data embedding techniques to improve prediction performance.\n",
    "\n",
    "def prepare_data(df):\n",
    "    dna_p = []\n",
    "    for i in range(df.shape[0]):\n",
    "        seg_p = df['DNA'][i]\n",
    "        idx_p_l = ['', 'A', 'C', 'G', 'T']\n",
    "        seg_p_l = []\n",
    "        for n in seg_p:\n",
    "            seg_p_l.append(idx_p_l.index(n))\n",
    "        dna_p.append(seg_p_l)\n",
    "\n",
    "    dna_p_t = torch.tensor(dna_p, dtype=torch.float32)\n",
    "    return dna_p_t"
   ],
   "id": "17800b2eefed6fcf",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T11:55:31.475677Z",
     "start_time": "2025-01-21T11:47:19.233268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 模型训练\n",
    "# 此处选取线性回归模型\n",
    "# 选手可充分考虑不同机器学习/深度学习模型，以提高预测效果\n",
    "# Model training here, the linear regression model is selected.\n",
    "# Participants are encouraged to fully consider different machine learning/deep learning models to improve prediction performance.\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dna, e_score):\n",
    "        super().__init__()\n",
    "        self.dna = dna\n",
    "        self.e_score = e_score\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dna)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dna[idx], self.e_score[idx]\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(8, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 16),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(16, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "dna = []\n",
    "e_scores = []\n",
    "\n",
    "for i in range(df_train.shape[0]):\n",
    "    seg = df_train['DNA'][i]\n",
    "    e_scores.append(df_train['E-score'][i])\n",
    "    idx_l = ['', 'A', 'C', 'G', 'T']\n",
    "    seg_l = []\n",
    "    for n in seg:\n",
    "        seg_l.append(idx_l.index(n))\n",
    "    dna.append(seg_l)\n",
    "\n",
    "dna_t = torch.tensor(dna, dtype=torch.float32)\n",
    "e_scores_t = torch.tensor(e_scores, dtype=torch.float32)\n",
    "\n",
    "dataset = MyDataset(dna_t, e_scores_t)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=512, shuffle=True)\n",
    "\n",
    "NUM_EPOCHS = 4000\n",
    "net = Net()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.002)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for dna_seg, e_score_seg in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = net(dna_seg)\n",
    "        loss = criterion(pred.T[0], e_score_seg)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        with torch.no_grad():\n",
    "            pred = net(dna_t)\n",
    "            loss = criterion(pred.T[0], e_scores_t)\n",
    "            print(f\"Epoch {epoch}, loss = {loss}\")"
   ],
   "id": "930e4490a5938b37",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss = 0.024050725623965263\n",
      "Epoch 10, loss = 0.01634918339550495\n",
      "Epoch 20, loss = 0.013934493996202946\n",
      "Epoch 30, loss = 0.013109150342643261\n",
      "Epoch 40, loss = 0.012429527007043362\n",
      "Epoch 50, loss = 0.011051633395254612\n",
      "Epoch 60, loss = 0.01002528890967369\n",
      "Epoch 70, loss = 0.009412321262061596\n",
      "Epoch 80, loss = 0.009113804437220097\n",
      "Epoch 90, loss = 0.008368595503270626\n",
      "Epoch 100, loss = 0.007940860465168953\n",
      "Epoch 110, loss = 0.007812554016709328\n",
      "Epoch 120, loss = 0.007434174884110689\n",
      "Epoch 130, loss = 0.007446628995239735\n",
      "Epoch 140, loss = 0.00737647945061326\n",
      "Epoch 150, loss = 0.006850802805274725\n",
      "Epoch 160, loss = 0.006693847011774778\n",
      "Epoch 170, loss = 0.0066839549690485\n",
      "Epoch 180, loss = 0.006337027996778488\n",
      "Epoch 190, loss = 0.006454660091549158\n",
      "Epoch 200, loss = 0.006064267363399267\n",
      "Epoch 210, loss = 0.005913172848522663\n",
      "Epoch 220, loss = 0.005727561190724373\n",
      "Epoch 230, loss = 0.005428430624306202\n",
      "Epoch 240, loss = 0.005575637798756361\n",
      "Epoch 250, loss = 0.005300300661474466\n",
      "Epoch 260, loss = 0.004981414880603552\n",
      "Epoch 270, loss = 0.004998754244297743\n",
      "Epoch 280, loss = 0.004685960244387388\n",
      "Epoch 290, loss = 0.004568944685161114\n",
      "Epoch 300, loss = 0.004419767763465643\n",
      "Epoch 310, loss = 0.004405356012284756\n",
      "Epoch 320, loss = 0.00425793556496501\n",
      "Epoch 330, loss = 0.0040036411955952644\n",
      "Epoch 340, loss = 0.003987524192780256\n",
      "Epoch 350, loss = 0.0037348170299082994\n",
      "Epoch 360, loss = 0.003697315463796258\n",
      "Epoch 370, loss = 0.0035591134801506996\n",
      "Epoch 380, loss = 0.0034716492518782616\n",
      "Epoch 390, loss = 0.003573899855837226\n",
      "Epoch 400, loss = 0.003360302187502384\n",
      "Epoch 410, loss = 0.0033198802266269922\n",
      "Epoch 420, loss = 0.0031677463557571173\n",
      "Epoch 430, loss = 0.0031374627724289894\n",
      "Epoch 440, loss = 0.0029252152889966965\n",
      "Epoch 450, loss = 0.0031064916402101517\n",
      "Epoch 460, loss = 0.0028724418953061104\n",
      "Epoch 470, loss = 0.002951868809759617\n",
      "Epoch 480, loss = 0.0027728076092898846\n",
      "Epoch 490, loss = 0.0026857967022806406\n",
      "Epoch 500, loss = 0.0025494429282844067\n",
      "Epoch 510, loss = 0.002552447374910116\n",
      "Epoch 520, loss = 0.002614851575344801\n",
      "Epoch 530, loss = 0.002488624071702361\n",
      "Epoch 540, loss = 0.0023600689601153135\n",
      "Epoch 550, loss = 0.002261927118524909\n",
      "Epoch 560, loss = 0.002262767869979143\n",
      "Epoch 570, loss = 0.002309161704033613\n",
      "Epoch 580, loss = 0.0022752240765839815\n",
      "Epoch 590, loss = 0.002158008748665452\n",
      "Epoch 600, loss = 0.0021492824889719486\n",
      "Epoch 610, loss = 0.0021025442983955145\n",
      "Epoch 620, loss = 0.0020668613724410534\n",
      "Epoch 630, loss = 0.0019415734568610787\n",
      "Epoch 640, loss = 0.002114385599270463\n",
      "Epoch 650, loss = 0.0019398510921746492\n",
      "Epoch 660, loss = 0.0018578644376248121\n",
      "Epoch 670, loss = 0.001954340375959873\n",
      "Epoch 680, loss = 0.0018696185434237123\n",
      "Epoch 690, loss = 0.0017758540343493223\n",
      "Epoch 700, loss = 0.0018100736197084188\n",
      "Epoch 710, loss = 0.0017888216534629464\n",
      "Epoch 720, loss = 0.001733155339024961\n",
      "Epoch 730, loss = 0.001743181492201984\n",
      "Epoch 740, loss = 0.0017568395705893636\n",
      "Epoch 750, loss = 0.0016913280123844743\n",
      "Epoch 760, loss = 0.0016744820168241858\n",
      "Epoch 770, loss = 0.0015547119546681643\n",
      "Epoch 780, loss = 0.001576079404912889\n",
      "Epoch 790, loss = 0.0014924961142241955\n",
      "Epoch 800, loss = 0.0015725047560408711\n",
      "Epoch 810, loss = 0.0016131954034790397\n",
      "Epoch 820, loss = 0.0015572088304907084\n",
      "Epoch 830, loss = 0.0015147655503824353\n",
      "Epoch 840, loss = 0.0014823017409071326\n",
      "Epoch 850, loss = 0.0014606430195271969\n",
      "Epoch 860, loss = 0.0014035656349733472\n",
      "Epoch 870, loss = 0.0014266102807596326\n",
      "Epoch 880, loss = 0.0014525826554745436\n",
      "Epoch 890, loss = 0.0013763790484517813\n",
      "Epoch 900, loss = 0.0013702315045520663\n",
      "Epoch 910, loss = 0.0014185950858518481\n",
      "Epoch 920, loss = 0.0014263383345678449\n",
      "Epoch 930, loss = 0.0014172467635944486\n",
      "Epoch 940, loss = 0.0014096662634983659\n",
      "Epoch 950, loss = 0.0013522198423743248\n",
      "Epoch 960, loss = 0.0012819054536521435\n",
      "Epoch 970, loss = 0.0014266968937590718\n",
      "Epoch 980, loss = 0.0014265620848163962\n",
      "Epoch 990, loss = 0.001221562153659761\n",
      "Epoch 1000, loss = 0.0013002565829083323\n",
      "Epoch 1010, loss = 0.0012444308958947659\n",
      "Epoch 1020, loss = 0.0012407191097736359\n",
      "Epoch 1030, loss = 0.0013019682373851538\n",
      "Epoch 1040, loss = 0.0012402619468048215\n",
      "Epoch 1050, loss = 0.001184077002108097\n",
      "Epoch 1060, loss = 0.001265334663912654\n",
      "Epoch 1070, loss = 0.0012348700547590852\n",
      "Epoch 1080, loss = 0.0012802635319530964\n",
      "Epoch 1090, loss = 0.0011854585027322173\n",
      "Epoch 1100, loss = 0.001227149972692132\n",
      "Epoch 1110, loss = 0.001158474711701274\n",
      "Epoch 1120, loss = 0.0011337865144014359\n",
      "Epoch 1130, loss = 0.001169406808912754\n",
      "Epoch 1140, loss = 0.00112200528383255\n",
      "Epoch 1150, loss = 0.0011573117226362228\n",
      "Epoch 1160, loss = 0.0011905061546713114\n",
      "Epoch 1170, loss = 0.0011617240961641073\n",
      "Epoch 1180, loss = 0.0011246348731219769\n",
      "Epoch 1190, loss = 0.0011011108290404081\n",
      "Epoch 1200, loss = 0.0010985853150486946\n",
      "Epoch 1210, loss = 0.0011015741620212793\n",
      "Epoch 1220, loss = 0.0010838430607691407\n",
      "Epoch 1230, loss = 0.0011799243511632085\n",
      "Epoch 1240, loss = 0.0011297855526208878\n",
      "Epoch 1250, loss = 0.0010685271117836237\n",
      "Epoch 1260, loss = 0.0011509695323184133\n",
      "Epoch 1270, loss = 0.0011135892709717155\n",
      "Epoch 1280, loss = 0.0010614977218210697\n",
      "Epoch 1290, loss = 0.0010399107122793794\n",
      "Epoch 1300, loss = 0.0010677094105631113\n",
      "Epoch 1310, loss = 0.0010863548377528787\n",
      "Epoch 1320, loss = 0.0011068361345678568\n",
      "Epoch 1330, loss = 0.0010567622957751155\n",
      "Epoch 1340, loss = 0.0010253742802888155\n",
      "Epoch 1350, loss = 0.0010802274337038398\n",
      "Epoch 1360, loss = 0.0010860890615731478\n",
      "Epoch 1370, loss = 0.0010878554312512279\n",
      "Epoch 1380, loss = 0.001050960156135261\n",
      "Epoch 1390, loss = 0.0010291737271472812\n",
      "Epoch 1400, loss = 0.001019208342768252\n",
      "Epoch 1410, loss = 0.0010002927156165242\n",
      "Epoch 1420, loss = 0.0010398081503808498\n",
      "Epoch 1430, loss = 0.0009783771820366383\n",
      "Epoch 1440, loss = 0.0011251018149778247\n",
      "Epoch 1450, loss = 0.00098294613417238\n",
      "Epoch 1460, loss = 0.0010065047536045313\n",
      "Epoch 1470, loss = 0.0010064218658953905\n",
      "Epoch 1480, loss = 0.0010560969822108746\n",
      "Epoch 1490, loss = 0.0010331072844564915\n",
      "Epoch 1500, loss = 0.0010524275712668896\n",
      "Epoch 1510, loss = 0.0009472546516917646\n",
      "Epoch 1520, loss = 0.0010484801605343819\n",
      "Epoch 1530, loss = 0.0009803911671042442\n",
      "Epoch 1540, loss = 0.0009200613130815327\n",
      "Epoch 1550, loss = 0.0010493758600205183\n",
      "Epoch 1560, loss = 0.0009749824530445039\n",
      "Epoch 1570, loss = 0.0010509637650102377\n",
      "Epoch 1580, loss = 0.0010126606794074178\n",
      "Epoch 1590, loss = 0.0010013083228841424\n",
      "Epoch 1600, loss = 0.0009535556891933084\n",
      "Epoch 1610, loss = 0.0009792001219466329\n",
      "Epoch 1620, loss = 0.0009305679704993963\n",
      "Epoch 1630, loss = 0.0009756772196851671\n",
      "Epoch 1640, loss = 0.0009166080853901803\n",
      "Epoch 1650, loss = 0.0009514133562333882\n",
      "Epoch 1660, loss = 0.0009645724203437567\n",
      "Epoch 1670, loss = 0.0010462075006216764\n",
      "Epoch 1680, loss = 0.0009018765413202345\n",
      "Epoch 1690, loss = 0.0008904121932573617\n",
      "Epoch 1700, loss = 0.0008802558295428753\n",
      "Epoch 1710, loss = 0.0009858974954113364\n",
      "Epoch 1720, loss = 0.0008793333545327187\n",
      "Epoch 1730, loss = 0.0009433521772734821\n",
      "Epoch 1740, loss = 0.0009554247953929007\n",
      "Epoch 1750, loss = 0.0009030279470607638\n",
      "Epoch 1760, loss = 0.000971103785559535\n",
      "Epoch 1770, loss = 0.0009205083479173481\n",
      "Epoch 1780, loss = 0.0008733401191420853\n",
      "Epoch 1790, loss = 0.0009100068127736449\n",
      "Epoch 1800, loss = 0.0008929575560614467\n",
      "Epoch 1810, loss = 0.0009165066876448691\n",
      "Epoch 1820, loss = 0.0009659583447501063\n",
      "Epoch 1830, loss = 0.0008776987669989467\n",
      "Epoch 1840, loss = 0.000914042757358402\n",
      "Epoch 1850, loss = 0.0009000615100376308\n",
      "Epoch 1860, loss = 0.0008981936844065785\n",
      "Epoch 1870, loss = 0.0008732913411222398\n",
      "Epoch 1880, loss = 0.0009396949899382889\n",
      "Epoch 1890, loss = 0.0009357009548693895\n",
      "Epoch 1900, loss = 0.0009239143109880388\n",
      "Epoch 1910, loss = 0.000864654837641865\n",
      "Epoch 1920, loss = 0.0009068117360584438\n",
      "Epoch 1930, loss = 0.0009087274083867669\n",
      "Epoch 1940, loss = 0.0009117598528973758\n",
      "Epoch 1950, loss = 0.0009665258112363517\n",
      "Epoch 1960, loss = 0.0009083512122742832\n",
      "Epoch 1970, loss = 0.0007997737848199904\n",
      "Epoch 1980, loss = 0.0008345088572241366\n",
      "Epoch 1990, loss = 0.0008189705549739301\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# def make_label(y, per=99):\n",
    "#     y = y.detach().numpy()\n",
    "#     threshold = np.percentile(y, per)\n",
    "#     labels = np.where(y >= threshold, 1, 0)\n",
    "#     return labels\n",
    "# x_train = prepare_data(df_train)\n",
    "# pred_train = make_label(net(x_train))\n",
    "# pd.DataFrame(pred_train).to_csv(\"submissionA.csv\", header = False, index = False)"
   ],
   "id": "78269d39c920144c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import zipfile\n",
    "# 模型预测, Model Prediction\n",
    "# 将连续值转化为01标签，Convert continuous values into 0-1 labels.\n",
    "\n",
    "def make_label(y, per=99):\n",
    "    y = y.detach().numpy()\n",
    "    threshold = np.percentile(y, per)\n",
    "    labels = np.where(y >= threshold, 1, 0)\n",
    "    return labels\n",
    "# 读取测试集数据，Read test set data.\n",
    "if os.environ.get('DATA_PATH'):\n",
    "        DATA_PATH = os.environ.get(\"DATA_PATH\") + \"/\"\n",
    "else:\n",
    "    print(\"Baseline运行时，因为无法读取测试集，所以会有此条报错，属于正常现象\")\n",
    "    print(\"When baseline is running, this error message will appear because the test set cannot be read, which is a normal phenomenon.\")\n",
    "    #Baseline运行时，因为无法读取测试集，所以会有此条报错，属于正常现象\n",
    "    #When baseline is running, this error message will appear because the test set cannot be read, which is a normal phenomenon.\n",
    "testA_path = DATA_PATH + \"testA.csv\"  #读取测试集A, read testing setA\n",
    "df_testA = pd.read_csv(testA_path)\n",
    "testB_path = DATA_PATH + \"testB.csv\" #读取测试集B,read teseting setB\n",
    "df_testB = pd.read_csv(testB_path)\n",
    "# A榜\n",
    "x_testA = prepare_data(df_testA)\n",
    "y_predA = make_label(net(x_testA))\n",
    "pd.DataFrame(y_predA).to_csv(\"submissionA.csv\", header = False, index = False)\n",
    "# B榜\n",
    "x_testB = prepare_data(df_testB)\n",
    "y_predB = make_label(net(x_testB))\n",
    "pd.DataFrame(y_predB).to_csv(\"submissionB.csv\", header = False, index = False)"
   ],
   "id": "cca7d88349580a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 定义要打包的文件和压缩文件名，Define the files to be packaged and the compressed file name.\n",
    "files_to_zip = ['submissionA.csv', 'submissionB.csv']\n",
    "zip_filename = 'submission.zip'\n",
    "\n",
    "# 创建一个 zip 文件，Create a zip file.\n",
    "with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
    "    for file in files_to_zip:\n",
    "        # 将文件添加到 zip 文件中，Add files to the zip file.\n",
    "        zipf.write(file, os.path.basename(file))\n",
    "\n",
    "print(f'{zip_filename} is created succefully!')"
   ],
   "id": "49f89b21a7f38bd6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
