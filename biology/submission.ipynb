{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 导入需要的包\n",
    "# Import the required packages.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 数据读取 input data\n",
    "# train_dir = \"/bohr/dataset-sxb8/v1/train.csv\"\n",
    "train_dir = \"train.csv\"\n",
    "df_train = pd.read_csv(train_dir)"
   ],
   "id": "45eb55928e9b5b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 数据预处理与数据嵌入；Data Preprocessing and Data Embedding\n",
    "# 由于该过程在预测集上也需要进行，此处将其整理为函数；Since this process also needs to be performed on the prediction set, it is organized into a function here.\n",
    "# 选手可充分考虑不同数据嵌入技术，以提高预测效果；Participants are encouraged to fully consider different data embedding techniques to improve prediction performance.\n",
    "\n",
    "def prepare_data(df):\n",
    "    dna_p = []\n",
    "    for i in range(df.shape[0]):\n",
    "        seg_p = df['DNA'][i]\n",
    "        idx_p_l = ['', 'A', 'C', 'G', 'T']\n",
    "        seg_p_l = []\n",
    "        for n in seg_p:\n",
    "            for j in range(4):\n",
    "                seg_p_l.append(0)\n",
    "            seg_p_l[-1 * idx_p_l.index(n)] = 1\n",
    "        dna_p.append(seg_p_l)\n",
    "\n",
    "    dna_p_t = torch.tensor(dna_p, dtype=torch.float32)\n",
    "    return dna_p_t"
   ],
   "id": "17800b2eefed6fcf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:43:46.907565Z",
     "start_time": "2025-01-23T04:20:12.551230Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 模型训练\n",
    "# 此处选取线性回归模型\n",
    "# 选手可充分考虑不同机器学习/深度学习模型，以提高预测效果\n",
    "# Model training here, the linear regression model is selected.\n",
    "# Participants are encouraged to fully consider different machine learning/deep learning models to improve prediction performance.\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dna, e_score):\n",
    "        super().__init__()\n",
    "        self.dna = dna\n",
    "        self.e_score = e_score\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dna)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dna[idx], self.e_score[idx]\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(32, 128, bias=False),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 16),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(16, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "dna = []\n",
    "e_scores = []\n",
    "\n",
    "for i in range(df_train.shape[0]):\n",
    "    seg = df_train['DNA'][i]\n",
    "    e_scores.append(df_train['E-score'][i])\n",
    "    idx_l = ['', 'A', 'C', 'G', 'T']\n",
    "    seg_l = []\n",
    "    for n in seg:\n",
    "        for j in range(4):\n",
    "            seg_l.append(0)\n",
    "        seg_l[-1 * idx_l.index(n)] = 1\n",
    "    dna.append(seg_l)\n",
    "\n",
    "dna_t = torch.tensor(dna, dtype=torch.float32)\n",
    "e_scores_t = torch.tensor(e_scores, dtype=torch.float32)\n",
    "\n",
    "dataset = MyDataset(dna_t, e_scores_t)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=1024, shuffle=True)\n",
    "\n",
    "NUM_EPOCHS = 5000\n",
    "net = Net()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for dna_seg, e_score_seg in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = net(dna_seg)\n",
    "        loss = criterion(pred.T[0], e_score_seg)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        with torch.no_grad():\n",
    "            pred = net(dna_t)\n",
    "            loss = criterion(pred.T[0], e_scores_t)\n",
    "            print(f\"Epoch {epoch}, loss = {loss}\")"
   ],
   "id": "930e4490a5938b37",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss = 0.022680697962641716\n",
      "Epoch 10, loss = 0.016368668526411057\n",
      "Epoch 20, loss = 0.010363459587097168\n",
      "Epoch 30, loss = 0.008740736171603203\n",
      "Epoch 40, loss = 0.007909799925982952\n",
      "Epoch 50, loss = 0.007871856912970543\n",
      "Epoch 60, loss = 0.007059202995151281\n",
      "Epoch 70, loss = 0.007058012764900923\n",
      "Epoch 80, loss = 0.006624286528676748\n",
      "Epoch 90, loss = 0.006581409834325314\n",
      "Epoch 100, loss = 0.0061463480815291405\n",
      "Epoch 110, loss = 0.005885763093829155\n",
      "Epoch 120, loss = 0.005719433538615704\n",
      "Epoch 130, loss = 0.005346465855836868\n",
      "Epoch 140, loss = 0.005176695995032787\n",
      "Epoch 150, loss = 0.004797676112502813\n",
      "Epoch 160, loss = 0.0047645713202655315\n",
      "Epoch 170, loss = 0.004165655001997948\n",
      "Epoch 180, loss = 0.003709051525220275\n",
      "Epoch 190, loss = 0.0033897170796990395\n",
      "Epoch 200, loss = 0.00303491298109293\n",
      "Epoch 210, loss = 0.0026368640828877687\n",
      "Epoch 220, loss = 0.002342302817851305\n",
      "Epoch 230, loss = 0.002049637259915471\n",
      "Epoch 240, loss = 0.0017293008277192712\n",
      "Epoch 250, loss = 0.0014742948114871979\n",
      "Epoch 260, loss = 0.0011696971487253904\n",
      "Epoch 270, loss = 0.0010833486448973417\n",
      "Epoch 280, loss = 0.0009151471895165741\n",
      "Epoch 290, loss = 0.0007065057288855314\n",
      "Epoch 300, loss = 0.0006231070728972554\n",
      "Epoch 310, loss = 0.00046063654008321464\n",
      "Epoch 320, loss = 0.00037749571492895484\n",
      "Epoch 330, loss = 0.0003264442493673414\n",
      "Epoch 340, loss = 0.00034853850957006216\n",
      "Epoch 350, loss = 0.0002032201155088842\n",
      "Epoch 360, loss = 0.00017449665756430477\n",
      "Epoch 370, loss = 0.0001569650776218623\n",
      "Epoch 380, loss = 0.00013025812222622335\n",
      "Epoch 390, loss = 0.00011630218068603426\n",
      "Epoch 400, loss = 0.00010731608199421316\n",
      "Epoch 410, loss = 7.555824413429946e-05\n",
      "Epoch 420, loss = 9.42170518101193e-05\n",
      "Epoch 430, loss = 6.776070222258568e-05\n",
      "Epoch 440, loss = 9.316042996942997e-05\n",
      "Epoch 450, loss = 7.524347893195227e-05\n",
      "Epoch 460, loss = 5.2762105042347685e-05\n",
      "Epoch 470, loss = 6.231685256352648e-05\n",
      "Epoch 480, loss = 0.00011012879986083135\n",
      "Epoch 490, loss = 5.167214840184897e-05\n",
      "Epoch 500, loss = 0.00017326624947600067\n",
      "Epoch 510, loss = 0.00011355877359164879\n",
      "Epoch 520, loss = 3.646963887149468e-05\n",
      "Epoch 530, loss = 6.951240357011557e-05\n",
      "Epoch 540, loss = 9.720434172777459e-05\n",
      "Epoch 550, loss = 2.7828189558931626e-05\n",
      "Epoch 560, loss = 6.228083657333627e-05\n",
      "Epoch 570, loss = 6.034021862433292e-05\n",
      "Epoch 580, loss = 4.210010592942126e-05\n",
      "Epoch 590, loss = 5.981314825476147e-05\n",
      "Epoch 600, loss = 7.402413757517934e-05\n",
      "Epoch 610, loss = 4.128666842007078e-05\n",
      "Epoch 620, loss = 8.115543460007757e-05\n",
      "Epoch 630, loss = 4.182293196208775e-05\n",
      "Epoch 640, loss = 4.075879041920416e-05\n",
      "Epoch 650, loss = 6.987983942963183e-05\n",
      "Epoch 660, loss = 6.261480302782729e-05\n",
      "Epoch 670, loss = 3.9459660911234096e-05\n",
      "Epoch 680, loss = 6.0817656049039215e-05\n",
      "Epoch 690, loss = 8.450238965451717e-05\n",
      "Epoch 700, loss = 0.00013190683966968209\n",
      "Epoch 710, loss = 2.5228051526937634e-05\n",
      "Epoch 720, loss = 8.92498719622381e-05\n",
      "Epoch 730, loss = 9.557489829603583e-05\n",
      "Epoch 740, loss = 5.4467618610942736e-05\n",
      "Epoch 750, loss = 5.930197221459821e-05\n",
      "Epoch 760, loss = 7.331883534789085e-05\n",
      "Epoch 770, loss = 6.233894964680076e-05\n",
      "Epoch 780, loss = 5.640689778374508e-05\n",
      "Epoch 790, loss = 5.6709195632720366e-05\n",
      "Epoch 800, loss = 8.345791866304353e-05\n",
      "Epoch 810, loss = 3.9855473005445674e-05\n",
      "Epoch 820, loss = 2.51353885687422e-05\n",
      "Epoch 830, loss = 9.757056250236928e-05\n",
      "Epoch 840, loss = 5.1009104936383665e-05\n",
      "Epoch 850, loss = 5.8975729189114645e-05\n",
      "Epoch 860, loss = 6.764070712961257e-05\n",
      "Epoch 870, loss = 2.808217323035933e-05\n",
      "Epoch 880, loss = 7.91418642620556e-05\n",
      "Epoch 890, loss = 5.273899296298623e-05\n",
      "Epoch 900, loss = 3.950417158193886e-05\n",
      "Epoch 910, loss = 4.035550955450162e-05\n",
      "Epoch 920, loss = 4.545360570773482e-05\n",
      "Epoch 930, loss = 5.4196287237573415e-05\n",
      "Epoch 940, loss = 3.399818160687573e-05\n",
      "Epoch 950, loss = 3.423125235713087e-05\n",
      "Epoch 960, loss = 0.0001428888936061412\n",
      "Epoch 970, loss = 2.0709989257738926e-05\n",
      "Epoch 980, loss = 3.0132669053273275e-05\n",
      "Epoch 990, loss = 0.00010624807327985764\n",
      "Epoch 1000, loss = 4.5688109821639955e-05\n",
      "Epoch 1010, loss = 7.008975080680102e-05\n",
      "Epoch 1020, loss = 4.1168881580233574e-05\n",
      "Epoch 1030, loss = 4.144173362874426e-05\n",
      "Epoch 1040, loss = 5.014470298192464e-05\n",
      "Epoch 1050, loss = 5.476177466334775e-05\n",
      "Epoch 1060, loss = 5.7934728829422966e-05\n",
      "Epoch 1070, loss = 4.558877844829112e-05\n",
      "Epoch 1080, loss = 2.65113303612452e-05\n",
      "Epoch 1090, loss = 3.0716553737875074e-05\n",
      "Epoch 1100, loss = 9.11440365598537e-05\n",
      "Epoch 1110, loss = 9.642298391554505e-05\n",
      "Epoch 1120, loss = 5.4587268095929176e-05\n",
      "Epoch 1130, loss = 2.0436227714526467e-05\n",
      "Epoch 1140, loss = 0.00012072527169948444\n",
      "Epoch 1150, loss = 3.0588660592911765e-05\n",
      "Epoch 1160, loss = 3.60385311068967e-05\n",
      "Epoch 1170, loss = 4.862451896769926e-05\n",
      "Epoch 1180, loss = 4.3567684770096093e-05\n",
      "Epoch 1190, loss = 6.229917198652402e-05\n",
      "Epoch 1200, loss = 4.090249058208428e-05\n",
      "Epoch 1210, loss = 4.1759518353501335e-05\n",
      "Epoch 1220, loss = 3.367264798725955e-05\n",
      "Epoch 1230, loss = 6.555786967510357e-05\n",
      "Epoch 1240, loss = 2.8672124244621955e-05\n",
      "Epoch 1250, loss = 4.301358785596676e-05\n",
      "Epoch 1260, loss = 5.9481593780219555e-05\n",
      "Epoch 1270, loss = 0.00010356056736782193\n",
      "Epoch 1280, loss = 2.530209530959837e-05\n",
      "Epoch 1290, loss = 3.0566057830583304e-05\n",
      "Epoch 1300, loss = 5.379201684263535e-05\n",
      "Epoch 1310, loss = 6.730047607561573e-05\n",
      "Epoch 1320, loss = 3.953885243390687e-05\n",
      "Epoch 1330, loss = 4.0655871998751536e-05\n",
      "Epoch 1340, loss = 6.196319736773148e-05\n",
      "Epoch 1350, loss = 2.1258287233649753e-05\n",
      "Epoch 1360, loss = 6.690597365377471e-05\n",
      "Epoch 1370, loss = 2.9369173716986552e-05\n",
      "Epoch 1380, loss = 3.988767639384605e-05\n",
      "Epoch 1390, loss = 3.203983942512423e-05\n",
      "Epoch 1400, loss = 4.6345307055162266e-05\n",
      "Epoch 1410, loss = 5.509974289452657e-05\n",
      "Epoch 1420, loss = 1.876371788966935e-05\n",
      "Epoch 1430, loss = 3.57664939656388e-05\n",
      "Epoch 1440, loss = 4.7475048631895334e-05\n",
      "Epoch 1450, loss = 2.9025708499830216e-05\n",
      "Epoch 1460, loss = 3.18893653457053e-05\n",
      "Epoch 1470, loss = 3.5179280530428514e-05\n",
      "Epoch 1480, loss = 4.919221100863069e-05\n",
      "Epoch 1490, loss = 2.5163744794554077e-05\n",
      "Epoch 1500, loss = 6.0508969909278676e-05\n",
      "Epoch 1510, loss = 4.3885098421014845e-05\n",
      "Epoch 1520, loss = 4.259702836861834e-05\n",
      "Epoch 1530, loss = 2.786937511700671e-05\n",
      "Epoch 1540, loss = 4.205539880786091e-05\n",
      "Epoch 1550, loss = 3.9901231502881274e-05\n",
      "Epoch 1560, loss = 2.9068018193356693e-05\n",
      "Epoch 1570, loss = 4.8973703087540343e-05\n",
      "Epoch 1580, loss = 3.385708623682149e-05\n",
      "Epoch 1590, loss = 2.7946936825173907e-05\n",
      "Epoch 1600, loss = 5.218413207330741e-05\n",
      "Epoch 1610, loss = 2.792124723782763e-05\n",
      "Epoch 1620, loss = 1.8247988919029012e-05\n",
      "Epoch 1630, loss = 5.8836903917836025e-05\n",
      "Epoch 1640, loss = 3.84706181648653e-05\n",
      "Epoch 1650, loss = 3.080856549786404e-05\n",
      "Epoch 1660, loss = 4.002028072136454e-05\n",
      "Epoch 1670, loss = 6.835637759650126e-05\n",
      "Epoch 1680, loss = 2.9177585020079277e-05\n",
      "Epoch 1690, loss = 3.4602115192683414e-05\n",
      "Epoch 1700, loss = 3.871131775667891e-05\n",
      "Epoch 1710, loss = 3.445078982622363e-05\n",
      "Epoch 1720, loss = 5.03502888022922e-05\n",
      "Epoch 1730, loss = 6.989507528487593e-05\n",
      "Epoch 1740, loss = 1.4115245903667528e-05\n",
      "Epoch 1750, loss = 4.324104520492256e-05\n",
      "Epoch 1760, loss = 6.368874164763838e-05\n",
      "Epoch 1770, loss = 2.572837365732994e-05\n",
      "Epoch 1780, loss = 2.8102454962208867e-05\n",
      "Epoch 1790, loss = 4.5492131903301924e-05\n",
      "Epoch 1800, loss = 3.34509059030097e-05\n",
      "Epoch 1810, loss = 2.596856575109996e-05\n",
      "Epoch 1820, loss = 3.025797923328355e-05\n",
      "Epoch 1830, loss = 3.4738899557851255e-05\n",
      "Epoch 1840, loss = 3.257122079958208e-05\n",
      "Epoch 1850, loss = 3.893689063261263e-05\n",
      "Epoch 1860, loss = 2.118070187862031e-05\n",
      "Epoch 1870, loss = 2.7713862436939962e-05\n",
      "Epoch 1880, loss = 3.305361678940244e-05\n",
      "Epoch 1890, loss = 4.848891694564372e-05\n",
      "Epoch 1900, loss = 2.5217177608283237e-05\n",
      "Epoch 1910, loss = 3.806058884947561e-05\n",
      "Epoch 1920, loss = 3.025870864803437e-05\n",
      "Epoch 1930, loss = 5.841375605086796e-05\n",
      "Epoch 1940, loss = 1.9692612113431096e-05\n",
      "Epoch 1950, loss = 1.4363781701831613e-05\n",
      "Epoch 1960, loss = 5.3586198191624135e-05\n",
      "Epoch 1970, loss = 3.624316013883799e-05\n",
      "Epoch 1980, loss = 2.2272110072663054e-05\n",
      "Epoch 1990, loss = 5.603834870271385e-05\n",
      "Epoch 2000, loss = 4.2981260776286945e-05\n",
      "Epoch 2010, loss = 1.8958116925205104e-05\n",
      "Epoch 2020, loss = 3.052279134863056e-05\n",
      "Epoch 2030, loss = 6.414827657863498e-05\n",
      "Epoch 2040, loss = 2.3081311155692674e-05\n",
      "Epoch 2050, loss = 3.1866769859334454e-05\n",
      "Epoch 2060, loss = 2.5298710170318373e-05\n",
      "Epoch 2070, loss = 2.3928263544803485e-05\n",
      "Epoch 2080, loss = 4.6480592573061585e-05\n",
      "Epoch 2090, loss = 3.2868290873011574e-05\n",
      "Epoch 2100, loss = 2.1142392142792232e-05\n",
      "Epoch 2110, loss = 2.055510958598461e-05\n",
      "Epoch 2120, loss = 4.743932368000969e-05\n",
      "Epoch 2130, loss = 3.51266389770899e-05\n",
      "Epoch 2140, loss = 2.8378854040056467e-05\n",
      "Epoch 2150, loss = 2.446276084810961e-05\n",
      "Epoch 2160, loss = 2.042017331405077e-05\n",
      "Epoch 2170, loss = 2.9363962312345393e-05\n",
      "Epoch 2180, loss = 2.97728183795698e-05\n",
      "Epoch 2190, loss = 4.286431067157537e-05\n",
      "Epoch 2200, loss = 1.0574731277301908e-05\n",
      "Epoch 2210, loss = 2.3471922759199515e-05\n",
      "Epoch 2220, loss = 3.5372366255614907e-05\n",
      "Epoch 2230, loss = 5.146776675246656e-05\n",
      "Epoch 2240, loss = 2.2870881366543472e-05\n",
      "Epoch 2250, loss = 2.9287240977282636e-05\n",
      "Epoch 2260, loss = 2.824049624905456e-05\n",
      "Epoch 2270, loss = 2.4750859665800817e-05\n",
      "Epoch 2280, loss = 4.3270294554531574e-05\n",
      "Epoch 2290, loss = 3.2728588848840445e-05\n",
      "Epoch 2300, loss = 3.225736509193666e-05\n",
      "Epoch 2310, loss = 1.582011282152962e-05\n",
      "Epoch 2320, loss = 2.2941245333640836e-05\n",
      "Epoch 2330, loss = 2.985999708471354e-05\n",
      "Epoch 2340, loss = 2.317604230483994e-05\n",
      "Epoch 2350, loss = 2.4776732971076854e-05\n",
      "Epoch 2360, loss = 2.2849260858492926e-05\n",
      "Epoch 2370, loss = 3.1183000828605145e-05\n",
      "Epoch 2380, loss = 4.66152596345637e-05\n",
      "Epoch 2390, loss = 1.9902234271285124e-05\n",
      "Epoch 2400, loss = 2.0591129214153625e-05\n",
      "Epoch 2410, loss = 3.881912198266946e-05\n",
      "Epoch 2420, loss = 3.0566319765057415e-05\n",
      "Epoch 2430, loss = 2.5817262212513015e-05\n",
      "Epoch 2440, loss = 3.797190947807394e-05\n",
      "Epoch 2450, loss = 1.9880464606103487e-05\n",
      "Epoch 2460, loss = 2.0100242181797512e-05\n",
      "Epoch 2470, loss = 8.36177496239543e-05\n",
      "Epoch 2480, loss = 2.231658982054796e-05\n",
      "Epoch 2490, loss = 1.4142684449325316e-05\n",
      "Epoch 2500, loss = 2.9332712074392475e-05\n",
      "Epoch 2510, loss = 4.4940308725927025e-05\n",
      "Epoch 2520, loss = 2.744739322224632e-05\n",
      "Epoch 2530, loss = 1.7801748981582932e-05\n",
      "Epoch 2540, loss = 2.563874477345962e-05\n",
      "Epoch 2550, loss = 4.2923795263050124e-05\n",
      "Epoch 2560, loss = 2.5336990802315995e-05\n",
      "Epoch 2570, loss = 1.5078844626259524e-05\n",
      "Epoch 2580, loss = 3.734403435373679e-05\n",
      "Epoch 2590, loss = 3.519956226227805e-05\n",
      "Epoch 2600, loss = 2.35064908338245e-05\n",
      "Epoch 2610, loss = 2.1251202269922942e-05\n",
      "Epoch 2620, loss = 4.790457751369104e-05\n",
      "Epoch 2630, loss = 1.9828383301501162e-05\n",
      "Epoch 2640, loss = 2.1370751710492186e-05\n",
      "Epoch 2650, loss = 3.491767347441055e-05\n",
      "Epoch 2660, loss = 3.917665526387282e-05\n",
      "Epoch 2670, loss = 1.4037931578059215e-05\n",
      "Epoch 2680, loss = 3.109453245997429e-05\n",
      "Epoch 2690, loss = 2.7476704417495057e-05\n",
      "Epoch 2700, loss = 2.7872212740476243e-05\n",
      "Epoch 2710, loss = 2.2866890503792092e-05\n",
      "Epoch 2720, loss = 1.7664044207776897e-05\n",
      "Epoch 2730, loss = 3.07350856019184e-05\n",
      "Epoch 2740, loss = 4.811932740267366e-05\n",
      "Epoch 2750, loss = 1.2212282854306977e-05\n",
      "Epoch 2760, loss = 1.6507487089256756e-05\n",
      "Epoch 2770, loss = 3.2828822440933436e-05\n",
      "Epoch 2780, loss = 2.746617428783793e-05\n",
      "Epoch 2790, loss = 2.6599172997521237e-05\n",
      "Epoch 2800, loss = 3.101590846199542e-05\n",
      "Epoch 2810, loss = 2.478960232110694e-05\n",
      "Epoch 2820, loss = 2.175084409827832e-05\n",
      "Epoch 2830, loss = 3.219784048269503e-05\n",
      "Epoch 2840, loss = 2.4227998437709175e-05\n",
      "Epoch 2850, loss = 2.8739896151819266e-05\n",
      "Epoch 2860, loss = 3.3902135328389704e-05\n",
      "Epoch 2870, loss = 9.54301231104182e-06\n",
      "Epoch 2880, loss = 2.7643107387120835e-05\n",
      "Epoch 2890, loss = 3.3293006708845496e-05\n",
      "Epoch 2900, loss = 3.225360705982894e-05\n",
      "Epoch 2910, loss = 2.981386387546081e-05\n",
      "Epoch 2920, loss = 3.626319085014984e-05\n",
      "Epoch 2930, loss = 3.0403087293962017e-05\n",
      "Epoch 2940, loss = 1.6460113329230808e-05\n",
      "Epoch 2950, loss = 2.675314863154199e-05\n",
      "Epoch 2960, loss = 3.162015855195932e-05\n",
      "Epoch 2970, loss = 1.676380452408921e-05\n",
      "Epoch 2980, loss = 1.7838037820183672e-05\n",
      "Epoch 2990, loss = 2.925715125456918e-05\n",
      "Epoch 3000, loss = 3.869059946737252e-05\n",
      "Epoch 3010, loss = 1.5637264368706383e-05\n",
      "Epoch 3020, loss = 2.5917679522535764e-05\n",
      "Epoch 3030, loss = 4.8345711547881365e-05\n",
      "Epoch 3040, loss = 2.7048638003179803e-05\n",
      "Epoch 3050, loss = 1.3997117093822453e-05\n",
      "Epoch 3060, loss = 2.3412145310430788e-05\n",
      "Epoch 3070, loss = 2.8691916668321937e-05\n",
      "Epoch 3080, loss = 3.186056710546836e-05\n",
      "Epoch 3090, loss = 1.6934232917265035e-05\n",
      "Epoch 3100, loss = 2.53373527812073e-05\n",
      "Epoch 3110, loss = 2.668566412467044e-05\n",
      "Epoch 3120, loss = 3.040563933609519e-05\n",
      "Epoch 3130, loss = 3.0355728085851297e-05\n",
      "Epoch 3140, loss = 3.712732359417714e-05\n",
      "Epoch 3150, loss = 1.9134857211611234e-05\n",
      "Epoch 3160, loss = 3.207464033039287e-05\n",
      "Epoch 3170, loss = 3.062062751268968e-05\n",
      "Epoch 3180, loss = 2.8973074222449213e-05\n",
      "Epoch 3190, loss = 1.3364534424908925e-05\n",
      "Epoch 3200, loss = 3.773366552195512e-05\n",
      "Epoch 3210, loss = 2.4627939637866803e-05\n",
      "Epoch 3220, loss = 1.6510986824869178e-05\n",
      "Epoch 3230, loss = 2.5042154447874054e-05\n",
      "Epoch 3240, loss = 2.48763280978892e-05\n",
      "Epoch 3250, loss = 1.4932145859347656e-05\n",
      "Epoch 3260, loss = 2.5272081984439865e-05\n",
      "Epoch 3270, loss = 2.153101195290219e-05\n",
      "Epoch 3280, loss = 1.5754867490613833e-05\n",
      "Epoch 3290, loss = 1.813147173379548e-05\n",
      "Epoch 3300, loss = 4.0409122448181733e-05\n",
      "Epoch 3310, loss = 9.264151231036521e-06\n",
      "Epoch 3320, loss = 2.8963004297111183e-05\n",
      "Epoch 3330, loss = 2.4197148377425037e-05\n",
      "Epoch 3340, loss = 1.9870594769599847e-05\n",
      "Epoch 3350, loss = 3.107913653366268e-05\n",
      "Epoch 3360, loss = 2.6259347578161396e-05\n",
      "Epoch 3370, loss = 2.8112735890317708e-05\n",
      "Epoch 3380, loss = 1.830116343626287e-05\n",
      "Epoch 3390, loss = 1.8796223230310716e-05\n",
      "Epoch 3400, loss = 2.4663737349328585e-05\n",
      "Epoch 3410, loss = 2.1539590306929313e-05\n",
      "Epoch 3420, loss = 3.278375879744999e-05\n",
      "Epoch 3430, loss = 1.673796032264363e-05\n",
      "Epoch 3440, loss = 1.790528949641157e-05\n",
      "Epoch 3450, loss = 3.091409962507896e-05\n",
      "Epoch 3460, loss = 2.442608638375532e-05\n",
      "Epoch 3470, loss = 1.8798136807163246e-05\n",
      "Epoch 3480, loss = 1.4859608199913055e-05\n",
      "Epoch 3490, loss = 2.1724545149481855e-05\n",
      "Epoch 3500, loss = 2.3635428078705445e-05\n",
      "Epoch 3510, loss = 2.1422891222755425e-05\n",
      "Epoch 3520, loss = 2.23009119508788e-05\n",
      "Epoch 3530, loss = 1.6386169590987265e-05\n",
      "Epoch 3540, loss = 2.974118251586333e-05\n",
      "Epoch 3550, loss = 2.61448494711658e-05\n",
      "Epoch 3560, loss = 2.679221324797254e-05\n",
      "Epoch 3570, loss = 1.7724592908052728e-05\n",
      "Epoch 3580, loss = 9.565290383761749e-06\n",
      "Epoch 3590, loss = 2.6292778784409165e-05\n",
      "Epoch 3600, loss = 3.088121593464166e-05\n",
      "Epoch 3610, loss = 3.9590395317645743e-05\n",
      "Epoch 3620, loss = 1.1394781722628977e-05\n",
      "Epoch 3630, loss = 1.1245905625401065e-05\n",
      "Epoch 3640, loss = 2.6577437893138267e-05\n",
      "Epoch 3650, loss = 3.193833981640637e-05\n",
      "Epoch 3660, loss = 1.710532342258375e-05\n",
      "Epoch 3670, loss = 2.1469886632985435e-05\n",
      "Epoch 3680, loss = 1.3785823284706566e-05\n",
      "Epoch 3690, loss = 2.6486846763873473e-05\n",
      "Epoch 3700, loss = 2.5060473490157165e-05\n",
      "Epoch 3710, loss = 1.6329739082721062e-05\n",
      "Epoch 3720, loss = 1.9817074644379318e-05\n",
      "Epoch 3730, loss = 2.6458048523636535e-05\n",
      "Epoch 3740, loss = 1.6911317288759165e-05\n",
      "Epoch 3750, loss = 1.9916951714549214e-05\n",
      "Epoch 3760, loss = 2.4617136659799144e-05\n",
      "Epoch 3770, loss = 2.211349237768445e-05\n",
      "Epoch 3780, loss = 1.6924710507737473e-05\n",
      "Epoch 3790, loss = 2.5931440177373588e-05\n",
      "Epoch 3800, loss = 2.5782665034057572e-05\n",
      "Epoch 3810, loss = 1.1912602531083394e-05\n",
      "Epoch 3820, loss = 1.941906157298945e-05\n",
      "Epoch 3830, loss = 2.2875387003296055e-05\n",
      "Epoch 3840, loss = 1.8794908100971952e-05\n",
      "Epoch 3850, loss = 2.5534103770041838e-05\n",
      "Epoch 3860, loss = 1.9324745153426193e-05\n",
      "Epoch 3870, loss = 1.8366479707765393e-05\n",
      "Epoch 3880, loss = 2.6615300157573074e-05\n",
      "Epoch 3890, loss = 2.7446518288343213e-05\n",
      "Epoch 3900, loss = 2.599780600576196e-05\n",
      "Epoch 3910, loss = 1.1868655747093726e-05\n",
      "Epoch 3920, loss = 1.3508612937584985e-05\n",
      "Epoch 3930, loss = 6.168623804114759e-05\n",
      "Epoch 3940, loss = 1.4497338270302862e-05\n",
      "Epoch 3950, loss = 9.369772669742815e-06\n",
      "Epoch 3960, loss = 3.0089675419731066e-05\n",
      "Epoch 3970, loss = 2.7691030481946655e-05\n",
      "Epoch 3980, loss = 1.6517684343853034e-05\n",
      "Epoch 3990, loss = 1.4474022464128211e-05\n",
      "Epoch 4000, loss = 3.625803583418019e-05\n",
      "Epoch 4010, loss = 1.6434236385975964e-05\n",
      "Epoch 4020, loss = 7.608010946569266e-06\n",
      "Epoch 4030, loss = 3.0147195502649993e-05\n",
      "Epoch 4040, loss = 2.1155832655495033e-05\n",
      "Epoch 4050, loss = 1.1660315976769198e-05\n",
      "Epoch 4060, loss = 1.42911867442308e-05\n",
      "Epoch 4070, loss = 2.6237110432703048e-05\n",
      "Epoch 4080, loss = 1.998220795940142e-05\n",
      "Epoch 4090, loss = 1.8092334357788786e-05\n",
      "Epoch 4100, loss = 1.8359309251536615e-05\n",
      "Epoch 4110, loss = 3.238893987145275e-05\n",
      "Epoch 4120, loss = 1.2997929843550082e-05\n",
      "Epoch 4130, loss = 1.121002424042672e-05\n",
      "Epoch 4140, loss = 3.468937211437151e-05\n",
      "Epoch 4150, loss = 1.8348855519434437e-05\n",
      "Epoch 4160, loss = 2.8676926376647316e-05\n",
      "Epoch 4170, loss = 8.519860784872435e-06\n",
      "Epoch 4180, loss = 2.5663195629022084e-05\n",
      "Epoch 4190, loss = 3.541056867106818e-05\n",
      "Epoch 4200, loss = 9.452246558794286e-06\n",
      "Epoch 4210, loss = 1.1844366781588178e-05\n",
      "Epoch 4220, loss = 2.659575329744257e-05\n",
      "Epoch 4230, loss = 3.4993747249245644e-05\n",
      "Epoch 4240, loss = 9.803274224395864e-06\n",
      "Epoch 4250, loss = 9.195978236675728e-06\n",
      "Epoch 4260, loss = 2.8942951757926494e-05\n",
      "Epoch 4270, loss = 2.9747145163128152e-05\n",
      "Epoch 4280, loss = 2.1834617655258626e-05\n",
      "Epoch 4290, loss = 1.0415547876618803e-05\n",
      "Epoch 4300, loss = 2.384259641985409e-05\n",
      "Epoch 4310, loss = 1.7593783923075534e-05\n",
      "Epoch 4320, loss = 2.4761769964243285e-05\n",
      "Epoch 4330, loss = 1.5261945009115152e-05\n",
      "Epoch 4340, loss = 1.3383450095716398e-05\n",
      "Epoch 4350, loss = 2.330922507098876e-05\n",
      "Epoch 4360, loss = 1.4720862964168191e-05\n",
      "Epoch 4370, loss = 1.2673232959059533e-05\n",
      "Epoch 4380, loss = 1.9567514755181037e-05\n",
      "Epoch 4390, loss = 4.0980907215271145e-05\n",
      "Epoch 4400, loss = 1.386240910505876e-05\n",
      "Epoch 4410, loss = 1.479372258472722e-05\n",
      "Epoch 4420, loss = 2.717039933486376e-05\n",
      "Epoch 4430, loss = 1.7337131794192828e-05\n",
      "Epoch 4440, loss = 1.0168042535951827e-05\n",
      "Epoch 4450, loss = 2.6767038434627466e-05\n",
      "Epoch 4460, loss = 1.954203617060557e-05\n",
      "Epoch 4470, loss = 1.0912117431871593e-05\n",
      "Epoch 4480, loss = 1.196868106490001e-05\n",
      "Epoch 4490, loss = 2.5164399630739354e-05\n",
      "Epoch 4500, loss = 1.8660808564163744e-05\n",
      "Epoch 4510, loss = 1.161075215350138e-05\n",
      "Epoch 4520, loss = 1.7542648492963053e-05\n",
      "Epoch 4530, loss = 2.0867419152637012e-05\n",
      "Epoch 4540, loss = 1.4993038348620757e-05\n",
      "Epoch 4550, loss = 1.0942650078504812e-05\n",
      "Epoch 4560, loss = 2.5712439310154878e-05\n",
      "Epoch 4570, loss = 2.2793905372964218e-05\n",
      "Epoch 4580, loss = 7.764596375636756e-06\n",
      "Epoch 4590, loss = 1.2499142030719668e-05\n",
      "Epoch 4600, loss = 3.487405047053471e-05\n",
      "Epoch 4610, loss = 8.28404699859675e-06\n",
      "Epoch 4620, loss = 1.5099360098247416e-05\n",
      "Epoch 4630, loss = 1.874851659522392e-05\n",
      "Epoch 4640, loss = 1.7120761185651645e-05\n",
      "Epoch 4650, loss = 1.659858025959693e-05\n",
      "Epoch 4660, loss = 1.6799331206129864e-05\n",
      "Epoch 4670, loss = 2.6371813873993233e-05\n",
      "Epoch 4680, loss = 9.895768926071469e-06\n",
      "Epoch 4690, loss = 1.7437132555642165e-05\n",
      "Epoch 4700, loss = 2.313670120202005e-05\n",
      "Epoch 4710, loss = 1.3797750398225617e-05\n",
      "Epoch 4720, loss = 1.7336828022962436e-05\n",
      "Epoch 4730, loss = 3.011652734130621e-05\n",
      "Epoch 4740, loss = 1.0005936019297224e-05\n",
      "Epoch 4750, loss = 2.2330134015646763e-05\n",
      "Epoch 4760, loss = 2.1641300918417983e-05\n",
      "Epoch 4770, loss = 1.5497495041927323e-05\n",
      "Epoch 4780, loss = 1.1625680599536281e-05\n",
      "Epoch 4790, loss = 1.3245994523458648e-05\n",
      "Epoch 4800, loss = 4.634584911400452e-05\n",
      "Epoch 4810, loss = 1.4441076018556487e-05\n",
      "Epoch 4820, loss = 8.39609401737107e-06\n",
      "Epoch 4830, loss = 2.4217268219217658e-05\n",
      "Epoch 4840, loss = 2.3506219804403372e-05\n",
      "Epoch 4850, loss = 8.595926374255214e-06\n",
      "Epoch 4860, loss = 1.5425754099851474e-05\n",
      "Epoch 4870, loss = 2.643909647304099e-05\n",
      "Epoch 4880, loss = 1.9531835278030485e-05\n",
      "Epoch 4890, loss = 1.9176142814103514e-05\n",
      "Epoch 4900, loss = 1.155319932877319e-05\n",
      "Epoch 4910, loss = 2.08089313673554e-05\n",
      "Epoch 4920, loss = 1.2568140846269671e-05\n",
      "Epoch 4930, loss = 1.3103038327244576e-05\n",
      "Epoch 4940, loss = 3.116366497124545e-05\n",
      "Epoch 4950, loss = 8.74722536536865e-06\n",
      "Epoch 4960, loss = 1.1158442248415668e-05\n",
      "Epoch 4970, loss = 2.1042556909378618e-05\n",
      "Epoch 4980, loss = 1.601085750735365e-05\n",
      "Epoch 4990, loss = 1.8338752852287143e-05\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T05:01:23.886160Z",
     "start_time": "2025-01-23T05:01:23.695064Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def make_label(y, per=99):\n",
    "    y = y.detach().numpy()\n",
    "    threshold = np.percentile(y, per)\n",
    "    labels = np.where(y >= threshold, 1, 0)\n",
    "    return labels\n",
    "x_train = prepare_data(df_train)\n",
    "pred_train = make_label(net(x_train))\n",
    "pd.DataFrame(pred_train).to_csv(\"submissionA.csv\", header = False, index = False)"
   ],
   "id": "78269d39c920144c",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import zipfile\n",
    "# 模型预测, Model Prediction\n",
    "# 将连续值转化为01标签，Convert continuous values into 0-1 labels.\n",
    "\n",
    "def make_label(y, per=99):\n",
    "    y = y.detach().numpy()\n",
    "    threshold = np.percentile(y, per)\n",
    "    labels = np.where(y >= threshold, 1, 0)\n",
    "    return labels\n",
    "# 读取测试集数据，Read test set data.\n",
    "if os.environ.get('DATA_PATH'):\n",
    "        DATA_PATH = os.environ.get(\"DATA_PATH\") + \"/\"\n",
    "else:\n",
    "    print(\"Baseline运行时，因为无法读取测试集，所以会有此条报错，属于正常现象\")\n",
    "    print(\"When baseline is running, this error message will appear because the test set cannot be read, which is a normal phenomenon.\")\n",
    "    #Baseline运行时，因为无法读取测试集，所以会有此条报错，属于正常现象\n",
    "    #When baseline is running, this error message will appear because the test set cannot be read, which is a normal phenomenon.\n",
    "testA_path = DATA_PATH + \"testA.csv\"  #读取测试集A, read testing setA\n",
    "df_testA = pd.read_csv(testA_path)\n",
    "testB_path = DATA_PATH + \"testB.csv\" #读取测试集B,read teseting setB\n",
    "df_testB = pd.read_csv(testB_path)\n",
    "# A榜\n",
    "x_testA = prepare_data(df_testA)\n",
    "y_predA = make_label(net(x_testA))\n",
    "pd.DataFrame(y_predA).to_csv(\"submissionA.csv\", header = False, index = False)\n",
    "# B榜\n",
    "x_testB = prepare_data(df_testB)\n",
    "y_predB = make_label(net(x_testB))\n",
    "pd.DataFrame(y_predB).to_csv(\"submissionB.csv\", header = False, index = False)"
   ],
   "id": "cca7d88349580a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 定义要打包的文件和压缩文件名，Define the files to be packaged and the compressed file name.\n",
    "files_to_zip = ['submissionA.csv', 'submissionB.csv']\n",
    "zip_filename = 'submission.zip'\n",
    "\n",
    "# 创建一个 zip 文件，Create a zip file.\n",
    "with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
    "    for file in files_to_zip:\n",
    "        # 将文件添加到 zip 文件中，Add files to the zip file.\n",
    "        zipf.write(file, os.path.basename(file))\n",
    "\n",
    "print(f'{zip_filename} is created succefully!')"
   ],
   "id": "49f89b21a7f38bd6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
