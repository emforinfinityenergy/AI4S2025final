{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-20T08:54:23.348768Z",
     "start_time": "2025-01-20T08:54:23.336092Z"
    }
   },
   "source": [
    "# 导入需要的包\n",
    "# Import the required packages.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T08:54:43.444599Z",
     "start_time": "2025-01-20T08:54:43.431093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 数据读取 input data\n",
    "train_dir = \"/bohr/dataset-sxb8/v1/train.csv\"\n",
    "# train_dir = \"train.csv\"\n",
    "df_train = pd.read_csv(train_dir)"
   ],
   "id": "45eb55928e9b5b6",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T08:54:44.989131Z",
     "start_time": "2025-01-20T08:54:44.984128Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 数据预处理与数据嵌入；Data Preprocessing and Data Embedding\n",
    "# 由于该过程在预测集上也需要进行，此处将其整理为函数；Since this process also needs to be performed on the prediction set, it is organized into a function here.\n",
    "# 选手可充分考虑不同数据嵌入技术，以提高预测效果；Participants are encouraged to fully consider different data embedding techniques to improve prediction performance.\n",
    "\n",
    "def prepare_data(df):\n",
    "    for i in range(df.shape[0]):\n",
    "        seg = df['DNA'][i]\n",
    "        idx_l = ['', 'A', 'C', 'G', 'T']\n",
    "        seg_l = []\n",
    "        for n in seg:\n",
    "            seg_l.append(idx_l.index(n))\n",
    "        dna.append(seg_l)\n",
    "\n",
    "    dna_t = torch.tensor(dna, dtype=torch.float32)\n",
    "    return dna_t"
   ],
   "id": "17800b2eefed6fcf",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T09:02:18.728823Z",
     "start_time": "2025-01-20T08:54:47.659989Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 模型训练\n",
    "# 此处选取线性回归模型\n",
    "# 选手可充分考虑不同机器学习/深度学习模型，以提高预测效果\n",
    "# Model training here, the linear regression model is selected.\n",
    "# Participants are encouraged to fully consider different machine learning/deep learning models to improve prediction performance.\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dna, e_score):\n",
    "        super().__init__()\n",
    "        self.dna = dna\n",
    "        self.e_score = e_score\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dna)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dna[idx], self.e_score[idx]\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(8, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 16),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(16, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "dna = []\n",
    "e_scores = []\n",
    "\n",
    "for i in range(df_train.shape[0]):\n",
    "    seg = df_train['DNA'][i]\n",
    "    e_scores.append(df_train['E-score'][i])\n",
    "    idx_l = ['', 'A', 'C', 'G', 'T']\n",
    "    seg_l = []\n",
    "    for n in seg:\n",
    "        seg_l.append(idx_l.index(n))\n",
    "    dna.append(seg_l)\n",
    "\n",
    "dna_t = torch.tensor(dna, dtype=torch.float32)\n",
    "e_scores_t = torch.tensor(e_scores, dtype=torch.float32)\n",
    "\n",
    "dataset = MyDataset(dna_t, e_scores_t)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "NUM_EPOCHS = 500\n",
    "net = Net()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for dna_seg, e_score_seg in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = net(dna_seg)\n",
    "        loss = criterion(pred.T[0], e_score_seg)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        with torch.no_grad():\n",
    "            pred = net(dna_t)\n",
    "            loss = criterion(pred.T[0], e_scores_t)\n",
    "            print(f\"Epoch {epoch}, loss = {loss}\")"
   ],
   "id": "930e4490a5938b37",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss = 0.01857765018939972\n",
      "Epoch 10, loss = 0.013916683383286\n",
      "Epoch 20, loss = 0.012102792039513588\n",
      "Epoch 30, loss = 0.010693647898733616\n",
      "Epoch 40, loss = 0.00965876504778862\n",
      "Epoch 50, loss = 0.009177572093904018\n",
      "Epoch 60, loss = 0.008699782192707062\n",
      "Epoch 70, loss = 0.008557253517210484\n",
      "Epoch 80, loss = 0.008097860030829906\n",
      "Epoch 90, loss = 0.007916121743619442\n",
      "Epoch 100, loss = 0.00794250052422285\n",
      "Epoch 110, loss = 0.0075547583401203156\n",
      "Epoch 120, loss = 0.0075728450901806355\n",
      "Epoch 130, loss = 0.007366225589066744\n",
      "Epoch 140, loss = 0.007325200829654932\n",
      "Epoch 150, loss = 0.007075557019561529\n",
      "Epoch 160, loss = 0.007599414326250553\n",
      "Epoch 170, loss = 0.007013965398073196\n",
      "Epoch 180, loss = 0.007068799342960119\n",
      "Epoch 190, loss = 0.007168845273554325\n",
      "Epoch 200, loss = 0.006720400881022215\n",
      "Epoch 210, loss = 0.006702834740281105\n",
      "Epoch 220, loss = 0.006819236557930708\n",
      "Epoch 230, loss = 0.006791173480451107\n",
      "Epoch 240, loss = 0.006716417148709297\n",
      "Epoch 250, loss = 0.0066254050470888615\n",
      "Epoch 260, loss = 0.006876333151012659\n",
      "Epoch 270, loss = 0.006536731030791998\n",
      "Epoch 280, loss = 0.006605810951441526\n",
      "Epoch 290, loss = 0.0064569455571472645\n",
      "Epoch 300, loss = 0.006441273260861635\n",
      "Epoch 310, loss = 0.006404148880392313\n",
      "Epoch 320, loss = 0.006377114914357662\n",
      "Epoch 330, loss = 0.0065837730653584\n",
      "Epoch 340, loss = 0.006185081787407398\n",
      "Epoch 350, loss = 0.006163336802273989\n",
      "Epoch 360, loss = 0.006103214342147112\n",
      "Epoch 370, loss = 0.005994134116917849\n",
      "Epoch 380, loss = 0.0068009342066943645\n",
      "Epoch 390, loss = 0.005995110608637333\n",
      "Epoch 400, loss = 0.006025726906955242\n",
      "Epoch 410, loss = 0.005963189993053675\n",
      "Epoch 420, loss = 0.005862354766577482\n",
      "Epoch 430, loss = 0.0058455755934119225\n",
      "Epoch 440, loss = 0.00583767332136631\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import zipfile\n",
    "# 模型预测, Model Prediction\n",
    "# 将连续值转化为01标签，Convert continuous values into 0-1 labels.\n",
    "\n",
    "def make_label(y, per=95):\n",
    "    y = y.detach().numpy()\n",
    "    threshold = np.percentile(y, per)\n",
    "    labels = np.where(y >= threshold, 1, 0)\n",
    "    return labels\n",
    "# 读取测试集数据，Read test set data.\n",
    "if os.environ.get('DATA_PATH'):\n",
    "        DATA_PATH = os.environ.get(\"DATA_PATH\") + \"/\"\n",
    "else:\n",
    "    print(\"Baseline运行时，因为无法读取测试集，所以会有此条报错，属于正常现象\")\n",
    "    print(\"When baseline is running, this error message will appear because the test set cannot be read, which is a normal phenomenon.\")\n",
    "    #Baseline运行时，因为无法读取测试集，所以会有此条报错，属于正常现象\n",
    "    #When baseline is running, this error message will appear because the test set cannot be read, which is a normal phenomenon.\n",
    "testA_path = DATA_PATH + \"testA.csv\"  #读取测试集A, read testing setA\n",
    "df_testA = pd.read_csv(testA_path)\n",
    "testB_path = DATA_PATH + \"testB.csv\" #读取测试集B,read teseting setB\n",
    "df_testB = pd.read_csv(testB_path)\n",
    "# A榜\n",
    "x_testA = prepare_data(df_testA)\n",
    "y_predA = make_label(net(x_testA))\n",
    "pd.DataFrame(y_predA).to_csv(\"submissionA.csv\", header = False, index = False)\n",
    "# B榜\n",
    "x_testB = prepare_data(df_testB)\n",
    "y_predB = make_label(net(x_testB))\n",
    "pd.DataFrame(y_predB).to_csv(\"submissionB.csv\", header = False, index = False)"
   ],
   "id": "cca7d88349580a8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 定义要打包的文件和压缩文件名，Define the files to be packaged and the compressed file name.\n",
    "files_to_zip = ['submissionA.csv', 'submissionB.csv']\n",
    "zip_filename = 'submission.zip'\n",
    "\n",
    "# 创建一个 zip 文件，Create a zip file.\n",
    "with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
    "    for file in files_to_zip:\n",
    "        # 将文件添加到 zip 文件中，Add files to the zip file.\n",
    "        zipf.write(file, os.path.basename(file))\n",
    "\n",
    "print(f'{zip_filename} is created succefully!')"
   ],
   "id": "49f89b21a7f38bd6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
