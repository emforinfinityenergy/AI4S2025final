{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 导入需要的包\n",
    "# Import the required packages.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 数据读取 input data\n",
    "train_dir = \"/bohr/dataset-sxb8/v1/train.csv\"\n",
    "# train_dir = \"train.csv\"\n",
    "df_train = pd.read_csv(train_dir)"
   ],
   "id": "45eb55928e9b5b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 数据预处理与数据嵌入；Data Preprocessing and Data Embedding\n",
    "# 由于该过程在预测集上也需要进行，此处将其整理为函数；Since this process also needs to be performed on the prediction set, it is organized into a function here.\n",
    "# 选手可充分考虑不同数据嵌入技术，以提高预测效果；Participants are encouraged to fully consider different data embedding techniques to improve prediction performance.\n",
    "\n",
    "def prepare_data(df):\n",
    "    dna_p = []\n",
    "    for i in range(df.shape[0]):\n",
    "        seg_p = df['DNA'][i]\n",
    "        idx_p_l = ['', 'A', 'C', 'G', 'T']\n",
    "        seg_p_l = []\n",
    "        for n in seg_p:\n",
    "            seg_p_l.append(idx_p_l.index(n))\n",
    "        dna_p.append(seg_p_l)\n",
    "\n",
    "    dna_p_t = torch.tensor(dna_p, dtype=torch.float32)\n",
    "    return dna_p_t"
   ],
   "id": "17800b2eefed6fcf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 模型训练\n",
    "# 此处选取线性回归模型\n",
    "# 选手可充分考虑不同机器学习/深度学习模型，以提高预测效果\n",
    "# Model training here, the linear regression model is selected.\n",
    "# Participants are encouraged to fully consider different machine learning/deep learning models to improve prediction performance.\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dna, e_score):\n",
    "        super().__init__()\n",
    "        self.dna = dna\n",
    "        self.e_score = e_score\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dna)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dna[idx], self.e_score[idx]\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(8, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 16),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(16, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "dna = []\n",
    "e_scores = []\n",
    "\n",
    "for i in range(df_train.shape[0]):\n",
    "    seg = df_train['DNA'][i]\n",
    "    e_scores.append(df_train['E-score'][i])\n",
    "    idx_l = ['', 'A', 'C', 'G', 'T']\n",
    "    seg_l = []\n",
    "    for n in seg:\n",
    "        seg_l.append(idx_l.index(n))\n",
    "    dna.append(seg_l)\n",
    "\n",
    "dna_t = torch.tensor(dna, dtype=torch.float32)\n",
    "e_scores_t = torch.tensor(e_scores, dtype=torch.float32)\n",
    "\n",
    "dataset = MyDataset(dna_t, e_scores_t)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=512, shuffle=True)\n",
    "\n",
    "NUM_EPOCHS = 2000\n",
    "net = Net()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.002)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for dna_seg, e_score_seg in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = net(dna_seg)\n",
    "        loss = criterion(pred.T[0], e_score_seg)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        with torch.no_grad():\n",
    "            pred = net(dna_t)\n",
    "            loss = criterion(pred.T[0], e_scores_t)\n",
    "            print(f\"Epoch {epoch}, loss = {loss}\")"
   ],
   "id": "930e4490a5938b37",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# def make_label(y, per=99):\n",
    "#     y = y.detach().numpy()\n",
    "#     threshold = np.percentile(y, per)\n",
    "#     labels = np.where(y >= threshold, 1, 0)\n",
    "#     return labels\n",
    "# x_train = prepare_data(df_train)\n",
    "# pred_train = make_label(net(x_train))\n",
    "# pd.DataFrame(pred_train).to_csv(\"submissionA.csv\", header = False, index = False)"
   ],
   "id": "78269d39c920144c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import zipfile\n",
    "# 模型预测, Model Prediction\n",
    "# 将连续值转化为01标签，Convert continuous values into 0-1 labels.\n",
    "\n",
    "def make_label(y, per=99):\n",
    "    y = y.detach().numpy()\n",
    "    threshold = np.percentile(y, per)\n",
    "    labels = np.where(y >= threshold, 1, 0)\n",
    "    return labels\n",
    "# 读取测试集数据，Read test set data.\n",
    "if os.environ.get('DATA_PATH'):\n",
    "        DATA_PATH = os.environ.get(\"DATA_PATH\") + \"/\"\n",
    "else:\n",
    "    print(\"Baseline运行时，因为无法读取测试集，所以会有此条报错，属于正常现象\")\n",
    "    print(\"When baseline is running, this error message will appear because the test set cannot be read, which is a normal phenomenon.\")\n",
    "    #Baseline运行时，因为无法读取测试集，所以会有此条报错，属于正常现象\n",
    "    #When baseline is running, this error message will appear because the test set cannot be read, which is a normal phenomenon.\n",
    "testA_path = DATA_PATH + \"testA.csv\"  #读取测试集A, read testing setA\n",
    "df_testA = pd.read_csv(testA_path)\n",
    "testB_path = DATA_PATH + \"testB.csv\" #读取测试集B,read teseting setB\n",
    "df_testB = pd.read_csv(testB_path)\n",
    "# A榜\n",
    "x_testA = prepare_data(df_testA)\n",
    "y_predA = make_label(net(x_testA))\n",
    "pd.DataFrame(y_predA).to_csv(\"submissionA.csv\", header = False, index = False)\n",
    "# B榜\n",
    "x_testB = prepare_data(df_testB)\n",
    "y_predB = make_label(net(x_testB))\n",
    "pd.DataFrame(y_predB).to_csv(\"submissionB.csv\", header = False, index = False)"
   ],
   "id": "cca7d88349580a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 定义要打包的文件和压缩文件名，Define the files to be packaged and the compressed file name.\n",
    "files_to_zip = ['submissionA.csv', 'submissionB.csv']\n",
    "zip_filename = 'submission.zip'\n",
    "\n",
    "# 创建一个 zip 文件，Create a zip file.\n",
    "with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
    "    for file in files_to_zip:\n",
    "        # 将文件添加到 zip 文件中，Add files to the zip file.\n",
    "        zipf.write(file, os.path.basename(file))\n",
    "\n",
    "print(f'{zip_filename} is created succefully!')"
   ],
   "id": "49f89b21a7f38bd6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
